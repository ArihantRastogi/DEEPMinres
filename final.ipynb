{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgD5WNpeupYW",
        "outputId": "e0c30955-28db-408b-8a79-ddea7e500130"
      },
      "outputs": [],
      "source": [
        "!pip install numpy tensorflow matplotlib tqdm scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "USr0_Bitq1qB",
        "outputId": "51a5decb-3043-4b5a-ee53-a7bd13722f82"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Matrix dimensions\n",
        "n = 64  # 64x64 matrices\n",
        "\n",
        "# --------------------------------\n",
        "# Data Generation Functions\n",
        "# --------------------------------\n",
        "\n",
        "def generate_spd_matrix(n):\n",
        "    \"\"\"Generate a random Symmetric Positive Definite matrix of size n x n.\"\"\"\n",
        "    A = np.random.randn(n, n)\n",
        "    # Make it symmetric\n",
        "    A = (A + A.T) / 2\n",
        "    # Make it positive definite by adding n*I\n",
        "    A = A + n * np.eye(n)\n",
        "    return A\n",
        "\n",
        "def generate_training_data(num_samples, n, max_iter=20):\n",
        "    \"\"\"\n",
        "    Generate improved training data for the neural network.\n",
        "\n",
        "    Args:\n",
        "        num_samples: Number of training samples to generate\n",
        "        n: Dimension of matrices\n",
        "        max_iter: Maximum iterations for data collection\n",
        "\n",
        "    Returns:\n",
        "        X: Input features for neural network\n",
        "        y: Target outputs for neural network\n",
        "    \"\"\"\n",
        "    X = []  # Will store input features\n",
        "    y = []  # Will store target values (optimal coefficients)\n",
        "\n",
        "    # Generate matrices with varying condition numbers\n",
        "    # to ensure the model learns across different problem types\n",
        "    for _ in tqdm(range(num_samples), desc=\"Generating training data\"):\n",
        "        # Generate matrices with varying difficulty\n",
        "        if _ % 3 == 0:\n",
        "            # Easy problems - well-conditioned matrices\n",
        "            condition_number = np.random.uniform(1, 10)\n",
        "        elif _ % 3 == 1:\n",
        "            # Medium problems\n",
        "            condition_number = np.random.uniform(10, 50)\n",
        "        else:\n",
        "            # Hard problems - ill-conditioned matrices\n",
        "            condition_number = np.random.uniform(50, 200)\n",
        "\n",
        "        # Create matrix with controlled condition number\n",
        "        Q, _ = np.linalg.qr(np.random.randn(n, n))\n",
        "        eigenvalues = np.linspace(1, condition_number, n)\n",
        "        A = Q @ np.diag(eigenvalues) @ Q.T\n",
        "\n",
        "        # Make sure it's symmetric\n",
        "        A = (A + A.T) / 2\n",
        "\n",
        "        # Generate a random solution vector\n",
        "        x_true = np.random.randn(n)\n",
        "\n",
        "        # Compute right-hand side b = A*x_true\n",
        "        b = A @ x_true\n",
        "\n",
        "        # Initialize MINRES\n",
        "        x = np.zeros(n)\n",
        "        r = b - A @ x\n",
        "        r_norm = np.linalg.norm(r)\n",
        "        q_old = np.zeros(n)\n",
        "        q = r / r_norm\n",
        "\n",
        "        # Store q vectors and residuals for the neural network\n",
        "        q_vectors = [np.zeros(n), q]  # [q_{-1}, q_0]\n",
        "        r_vectors = [np.zeros(n), r]  # [r_{-1}, r_0]\n",
        "\n",
        "        # Run modified MINRES for data collection\n",
        "        for k in range(1, max_iter+1):\n",
        "            # Lanczos iteration with reorthogonalization\n",
        "            v = A @ q\n",
        "            alpha = np.dot(q, v)\n",
        "            v = v - alpha * q\n",
        "\n",
        "            # Reorthogonalize against previous vectors for stability\n",
        "            if k > 1:\n",
        "                v = v - np.dot(q_old, v) * q_old\n",
        "\n",
        "            beta_next = np.linalg.norm(v)\n",
        "            q_next = v / beta_next if beta_next > 1e-10 else np.zeros(n)\n",
        "\n",
        "            # Ensure we have enough history before collecting samples\n",
        "            if k >= 3:\n",
        "                # Collect the 3 most recent q vectors and residuals\n",
        "                recent_q = [q_vectors[-3], q_vectors[-2], q_vectors[-1]]\n",
        "                recent_r = [r_vectors[-3], r_vectors[-2], r_vectors[-1]]\n",
        "\n",
        "                # Compute the optimal coefficients using direct minimization\n",
        "                # This will be what our neural network aims to learn\n",
        "                coeffs = compute_optimal_coefficients(A, recent_q, r_vectors[-1])\n",
        "\n",
        "                # Normalize residuals to avoid scaling problems\n",
        "                r_scale = np.linalg.norm(r_vectors[-1]) + 1e-10\n",
        "\n",
        "                # Store input features and target outputs\n",
        "                features = np.concatenate([\n",
        "                    recent_q[0], recent_q[1], recent_q[2],\n",
        "                    r_vectors[-3]/r_scale, r_vectors[-2]/r_scale, r_vectors[-1]/r_scale\n",
        "                ])\n",
        "\n",
        "                # Only add training examples where coefficients would meaningfully improve convergence\n",
        "                p = coeffs[0] * recent_q[2] + coeffs[1] * recent_q[1] + coeffs[2] * recent_q[0]\n",
        "\n",
        "                # Check if this would be a good search direction\n",
        "                # by comparing with standard MINRES direction\n",
        "                std_p = recent_q[2]  # Standard MINRES would use the most recent q\n",
        "\n",
        "                # If our p gives a better reduction in residual, include this example\n",
        "                std_eta = compute_optimal_step(A, x, std_p, b)\n",
        "                std_reduction = np.linalg.norm(b - A @ (x + std_eta * std_p))\n",
        "\n",
        "                eta = compute_optimal_step(A, x, p, b)\n",
        "                our_reduction = np.linalg.norm(b - A @ (x + eta * p))\n",
        "\n",
        "                # Only use examples where our approach is actually better\n",
        "                if our_reduction < std_reduction * 0.99:  # At least 1% improvement\n",
        "                    X.append(features)\n",
        "                    y.append(coeffs)\n",
        "\n",
        "            # Update for next iteration\n",
        "            q_old = q\n",
        "            q = q_next\n",
        "            q_vectors.append(q)\n",
        "\n",
        "            # Update solution and residual to collect more realistic data\n",
        "            if k >= 3:\n",
        "                p = coeffs[0] * q_vectors[-1] + coeffs[1] * q_vectors[-2] + coeffs[2] * q_vectors[-3]\n",
        "                eta = compute_optimal_step(A, x, p, b)\n",
        "                x = x + eta * p\n",
        "                r = b - A @ x\n",
        "            else:\n",
        "                # For first iterations, use standard MINRES\n",
        "                p = q_vectors[-1]\n",
        "                eta = compute_optimal_step(A, x, p, b)\n",
        "                x = x + eta * p\n",
        "                r = b - A @ x\n",
        "\n",
        "            r_vectors.append(r)\n",
        "\n",
        "            # Early stopping if converged\n",
        "            if np.linalg.norm(r) < 1e-10:\n",
        "                break\n",
        "\n",
        "    # Ensure we have enough training samples\n",
        "    if len(X) < num_samples // 2:\n",
        "        print(f\"Warning: Only generated {len(X)} useful training samples. Collecting more...\")\n",
        "        return generate_training_data(num_samples * 2, n, max_iter)\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def compute_optimal_coefficients(A, q_vectors, r):\n",
        "    \"\"\"\n",
        "    Compute optimal coefficients for the search direction that better approximate\n",
        "    the direction of steepest descent in the A-norm.\n",
        "\n",
        "    Args:\n",
        "        A: System matrix\n",
        "        q_vectors: List of the three most recent q vectors [q_{k-2}, q_{k-1}, q_k]\n",
        "        r: Current residual\n",
        "\n",
        "    Returns:\n",
        "        coeffs: Three coefficients [alpha, beta, gamma]\n",
        "    \"\"\"\n",
        "    # Get the three most recent q vectors\n",
        "    q_k, q_k1, q_k2 = q_vectors[2], q_vectors[1], q_vectors[0]\n",
        "\n",
        "    # Create a matrix whose columns are the q vectors\n",
        "    Q = np.column_stack([q_k, q_k1, q_k2])\n",
        "\n",
        "    # Compute AQ\n",
        "    AQ = A @ Q\n",
        "\n",
        "    # Compute the Gram matrix Q^T * A * Q\n",
        "    gram = Q.T @ AQ\n",
        "\n",
        "    # Compute Q^T * r\n",
        "    Qtr = Q.T @ r\n",
        "\n",
        "    # Solve the small 3x3 system to find optimal coefficients\n",
        "    # that minimize ||A(x + p) - b||_2 where p is a linear combination of q vectors\n",
        "    try:\n",
        "        # Use least squares to handle potential rank-deficiency\n",
        "        coeffs = np.linalg.lstsq(gram, Qtr, rcond=1e-10)[0]\n",
        "    except np.linalg.LinAlgError:\n",
        "        # Fallback to a simpler approach if the matrix is singular\n",
        "        coeffs = np.array([\n",
        "            np.dot(r, q_k) / (np.dot(q_k, A @ q_k) + 1e-10),\n",
        "            np.dot(r, q_k1) / (np.dot(q_k1, A @ q_k1) + 1e-10),\n",
        "            np.dot(r, q_k2) / (np.dot(q_k2, A @ q_k2) + 1e-10)\n",
        "        ])\n",
        "\n",
        "    # Normalize the coefficients to improve stability\n",
        "    norm = np.linalg.norm(coeffs)\n",
        "    if norm > 1e-10:\n",
        "        coeffs = coeffs / norm\n",
        "\n",
        "    return coeffs\n",
        "\n",
        "def compute_optimal_step(A, x, p, b):\n",
        "    \"\"\"\n",
        "    Compute the optimal step size eta that minimizes ||A(x + eta*p) - b||_2.\n",
        "\n",
        "    Args:\n",
        "        A: System matrix\n",
        "        x: Current solution vector\n",
        "        p: Search direction\n",
        "        b: Right-hand side vector\n",
        "\n",
        "    Returns:\n",
        "        eta: Optimal step size\n",
        "    \"\"\"\n",
        "    Ap = A @ p\n",
        "    r = b - A @ x\n",
        "\n",
        "    # Compute optimal step size\n",
        "    eta = np.dot(r, Ap) / (np.dot(Ap, Ap) + 1e-10)\n",
        "\n",
        "    return eta\n",
        "\n",
        "# --------------------------------\n",
        "# Neural Network Model\n",
        "# --------------------------------\n",
        "\n",
        "def build_neural_network():\n",
        "    \"\"\"Build the neural network model G_Î¸ for DeepMINRES with improved architecture.\"\"\"\n",
        "\n",
        "    # Input: 6*n features - 3 q vectors (q_k, q_{k-1}, q_{k-2}) and 3 residuals (r_k, r_{k-1}, r_{k-2})\n",
        "    input_size = 6 * n\n",
        "\n",
        "    # Define the model architecture\n",
        "    # Use smaller network with regularization to prevent overfitting\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(input_size,)),\n",
        "        # First extract features from high-dimensional input\n",
        "        tf.keras.layers.Dense(512, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "\n",
        "        # Reduce dimensionality gradually\n",
        "        tf.keras.layers.Dense(256, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.1),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "\n",
        "        # Final output layer\n",
        "        tf.keras.layers.Dense(3, activation='tanh')  # Tanh ensures bounded coefficients\n",
        "    ])\n",
        "\n",
        "    # Compile the model with custom loss function that encourages\n",
        "    # the model to produce coefficients that reduce residual\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
        "        loss='mean_squared_error'\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# --------------------------------\n",
        "# DeepMINRES Algorithm\n",
        "# --------------------------------\n",
        "\n",
        "def deepminres(A, b, x0, model, max_iter=100, tol=1e-10):\n",
        "    \"\"\"\n",
        "    Implement the DeepMINRES algorithm with stability improvements.\n",
        "\n",
        "    Args:\n",
        "        A: System matrix (n x n)\n",
        "        b: Right-hand side vector (n)\n",
        "        x0: Initial guess (n)\n",
        "        model: Trained neural network model\n",
        "        max_iter: Maximum iterations\n",
        "        tol: Tolerance for convergence\n",
        "\n",
        "    Returns:\n",
        "        x: Solution vector\n",
        "        residual_norms: List of residual norms for each iteration\n",
        "    \"\"\"\n",
        "    n = len(b)\n",
        "    x = x0.copy()\n",
        "    r = b - A @ x\n",
        "    r_norm = np.linalg.norm(r)\n",
        "\n",
        "    # Initialize variables\n",
        "    residual_norms = [r_norm]\n",
        "\n",
        "    if r_norm < tol:\n",
        "        return x, residual_norms  # Initial guess is good enough\n",
        "\n",
        "    # Initial q vectors for Lanczos process\n",
        "    q_old2 = np.zeros(n)  # q_{-1}\n",
        "    q_old1 = np.zeros(n)  # q_0\n",
        "    q = r / r_norm  # q_1 (normalized residual)\n",
        "\n",
        "    # Initial residuals for network input\n",
        "    r_old2 = np.zeros(n)\n",
        "    r_old1 = np.zeros(n)\n",
        "\n",
        "    # Store vectors for orthogonality checks\n",
        "    q_vectors = [q_old1, q]  # Store recent q vectors\n",
        "\n",
        "    # For the first few iterations, use standard MINRES (more stable)\n",
        "    # until we have enough history for the neural network\n",
        "    for k in range(1, 3):\n",
        "        # Run one Lanczos iteration to obtain q_k\n",
        "        v = A @ q\n",
        "        alpha = np.dot(q, v)\n",
        "        v = v - alpha * q - (np.dot(q_old1, v) * q_old1 if k > 1 else 0)\n",
        "\n",
        "        beta = np.linalg.norm(v)\n",
        "        if beta < 1e-10:\n",
        "            # Breakdown in Lanczos - we've found an invariant subspace\n",
        "            # This is actually good - use the current q as search direction\n",
        "            p = q\n",
        "        else:\n",
        "            q_next = v / beta\n",
        "            # Explicitly re-orthogonalize\n",
        "            for prev_q in q_vectors:\n",
        "                q_next = q_next - np.dot(prev_q, q_next) * prev_q\n",
        "\n",
        "            q_next = q_next / np.linalg.norm(q_next) if np.linalg.norm(q_next) > 1e-10 else np.zeros(n)\n",
        "\n",
        "            # Use standard MINRES direction for first iterations\n",
        "            p = q\n",
        "\n",
        "        # Compute optimal step size\n",
        "        eta = compute_optimal_step(A, x, p, b)\n",
        "\n",
        "        # Update solution\n",
        "        x = x + eta * p\n",
        "\n",
        "        # Update residual\n",
        "        r_old2 = r_old1\n",
        "        r_old1 = r\n",
        "        r = b - A @ x\n",
        "        r_norm = np.linalg.norm(r)\n",
        "        residual_norms.append(r_norm)\n",
        "\n",
        "        # Check for convergence\n",
        "        if r_norm < tol:\n",
        "            print(f\"Converged after {k} iterations with residual norm {r_norm:.1e}\")\n",
        "            break\n",
        "\n",
        "        # Update for next iteration\n",
        "        q_old2 = q_old1\n",
        "        q_old1 = q\n",
        "        q = q_next\n",
        "        q_vectors.append(q)\n",
        "\n",
        "    # Main iteration loop with neural network\n",
        "    for k in range(3, max_iter+1):\n",
        "        # Run one Lanczos iteration to obtain q_k\n",
        "        v = A @ q\n",
        "        alpha = np.dot(q, v)\n",
        "        v = v - alpha * q - np.dot(q_old1, v) * q_old1\n",
        "\n",
        "        beta = np.linalg.norm(v)\n",
        "        if beta < 1e-10:\n",
        "            # Lanczos breakdown - use the current direction\n",
        "            p = q\n",
        "        else:\n",
        "            q_next = v / beta\n",
        "\n",
        "            # Explicit reorthogonalization for numerical stability\n",
        "            for prev_q in q_vectors[-3:]:  # Reorthogonalize against recent vectors\n",
        "                q_next = q_next - np.dot(prev_q, q_next) * prev_q\n",
        "\n",
        "            q_next = q_next / np.linalg.norm(q_next) if np.linalg.norm(q_next) > 1e-10 else np.zeros(n)\n",
        "\n",
        "            # Prepare input for the neural network - normalize to avoid scaling issues\n",
        "            recent_q = [q_old2, q_old1, q]\n",
        "            recent_r = [r_old2, r_old1, r]\n",
        "\n",
        "            # Feature normalization is important\n",
        "            r_scale = np.linalg.norm(r) + 1e-10\n",
        "            nn_input = np.concatenate([\n",
        "                q_old2, q_old1, q,  # These are already normalized\n",
        "                r_old2/r_scale, r_old1/r_scale, r/r_scale  # Normalize residuals\n",
        "            ])\n",
        "            nn_input = nn_input.reshape(1, -1)\n",
        "\n",
        "            # Get coefficients from the neural network\n",
        "            coeffs = model.predict(nn_input, verbose=0)[0]\n",
        "\n",
        "            # Compute search direction\n",
        "            p = coeffs[0] * q + coeffs[1] * q_old1 + coeffs[2] * q_old2\n",
        "\n",
        "            # Ensure non-zero search direction\n",
        "            p_norm = np.linalg.norm(p)\n",
        "            if p_norm < 1e-10:\n",
        "                # If neural network gives bad direction, fall back to standard MINRES\n",
        "                p = q\n",
        "            else:\n",
        "                p = p / p_norm  # Normalize search direction\n",
        "\n",
        "        # Compute optimal step size (this is crucial for performance)\n",
        "        eta = compute_optimal_step(A, x, p, b)\n",
        "\n",
        "        # Update solution\n",
        "        x = x + eta * p\n",
        "\n",
        "        # Update residual\n",
        "        r_old2 = r_old1\n",
        "        r_old1 = r\n",
        "        r = b - A @ x\n",
        "        r_norm = np.linalg.norm(r)\n",
        "        residual_norms.append(r_norm)\n",
        "\n",
        "        # Check for convergence\n",
        "        if r_norm < tol:\n",
        "            print(f\"Converged after {k} iterations with residual norm {r_norm:.1e}\")\n",
        "            break\n",
        "\n",
        "        # Update for next iteration\n",
        "        q_old2 = q_old1\n",
        "        q_old1 = q\n",
        "        q = q_next\n",
        "        q_vectors.append(q)\n",
        "\n",
        "        # Keep q_vectors at a reasonable size\n",
        "        if len(q_vectors) > 5:\n",
        "            q_vectors.pop(0)\n",
        "\n",
        "    return x, residual_norms\n",
        "\n",
        "# --------------------------------\n",
        "# Evaluate Algorithm\n",
        "# --------------------------------\n",
        "\n",
        "def evaluate_solver(solver_func, test_matrices, test_solutions, solver_name):\n",
        "    \"\"\"\n",
        "    Evaluate a linear system solver on test problems.\n",
        "\n",
        "    Args:\n",
        "        solver_func: Function that solves the linear system\n",
        "        test_matrices: List of test matrices\n",
        "        test_solutions: List of true solutions\n",
        "        solver_name: Name of the solver for display\n",
        "\n",
        "    Returns:\n",
        "        results: Dictionary with solver results\n",
        "    \"\"\"\n",
        "    num_tests = len(test_matrices)\n",
        "    iterations = []\n",
        "    times = []\n",
        "    rel_errors = []\n",
        "\n",
        "    for i in range(num_tests):\n",
        "        A = test_matrices[i]\n",
        "        x_true = test_solutions[i]\n",
        "        b = A @ x_true\n",
        "\n",
        "        # Initial guess\n",
        "        x0 = np.zeros_like(x_true)\n",
        "\n",
        "        # Time the solution\n",
        "        start_time = time.time()\n",
        "\n",
        "        if solver_name == \"DeepMINRES\":\n",
        "            x, res_norms = solver_func(A, b, x0, trained_model)\n",
        "        else:\n",
        "            x, res_norms = solver_func(A, b, x0)\n",
        "\n",
        "        solve_time = time.time() - start_time\n",
        "\n",
        "        # Compute error\n",
        "        rel_error = np.linalg.norm(x - x_true) / np.linalg.norm(x_true)\n",
        "\n",
        "        iterations.append(len(res_norms) - 1)\n",
        "        times.append(solve_time)\n",
        "        rel_errors.append(rel_error)\n",
        "\n",
        "    results = {\n",
        "        \"solver\": solver_name,\n",
        "        \"avg_iterations\": np.mean(iterations),\n",
        "        \"avg_time\": np.mean(times),\n",
        "        \"avg_rel_error\": np.mean(rel_errors)\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "# --------------------------------\n",
        "# Baseline MINRES Algorithm\n",
        "# --------------------------------\n",
        "\n",
        "def standard_minres(A, b, x0, max_iter=100, tol=1e-10):\n",
        "    \"\"\"\n",
        "    Standard MINRES algorithm for comparison.\n",
        "\n",
        "    Args:\n",
        "        A: System matrix (n x n)\n",
        "        b: Right-hand side vector (n)\n",
        "        x0: Initial guess (n)\n",
        "        max_iter: Maximum iterations\n",
        "        tol: Tolerance for convergence\n",
        "\n",
        "    Returns:\n",
        "        x: Solution vector\n",
        "        residual_norms: List of residual norms for each iteration\n",
        "    \"\"\"\n",
        "    n = len(b)\n",
        "    x = x0.copy()\n",
        "    r = b - A @ x\n",
        "    beta = np.linalg.norm(r)\n",
        "\n",
        "    # Initialize variables\n",
        "    residual_norms = [beta]\n",
        "\n",
        "    # Initial q vectors\n",
        "    q_old = np.zeros(n)\n",
        "    q = r / beta\n",
        "\n",
        "    for k in range(1, max_iter+1):\n",
        "        # Lanczos iteration\n",
        "        v = A @ q\n",
        "        alpha = np.dot(q, v)\n",
        "        v = v - alpha * q - beta * q_old\n",
        "        beta_next = np.linalg.norm(v)\n",
        "        q_next = v / beta_next if beta_next > 1e-10 else np.zeros(n)\n",
        "\n",
        "        # Standard MINRES update\n",
        "        p = q  # In standard MINRES, we only use the current q\n",
        "        eta = np.dot(r, A @ p) / (np.dot(A @ p, A @ p) + 1e-10)\n",
        "\n",
        "        # Update solution\n",
        "        x = x + eta * p\n",
        "\n",
        "        # Update residual\n",
        "        r = b - A @ x\n",
        "        residual_norm = np.linalg.norm(r)\n",
        "        residual_norms.append(residual_norm)\n",
        "\n",
        "        # Check for convergence\n",
        "        if residual_norm < tol:\n",
        "            print(f\"Standard MINRES converged after {k} iterations with residual norm {residual_norm:.1e}\")\n",
        "            break\n",
        "\n",
        "        # Update for next iteration\n",
        "        q_old = q\n",
        "        q = q_next\n",
        "        beta = beta_next\n",
        "\n",
        "    return x, residual_norms\n",
        "\n",
        "# --------------------------------\n",
        "# Main Execution\n",
        "# --------------------------------\n",
        "\n",
        "# Generate training data\n",
        "print(\"Step 1: Generating training data\")\n",
        "num_train_samples = 2000  # Increased for better coverage\n",
        "X_train, y_train = generate_training_data(num_train_samples, n, max_iter=30)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Build and train the neural network\n",
        "print(\"\\nStep 2: Building and training the neural network\")\n",
        "model = build_neural_network()\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Create early stopping and model checkpoint callbacks\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    'best_deepminres_model.h5',\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Learning rate scheduler\n",
        "def lr_scheduler(epoch, lr):\n",
        "    if epoch > 10:\n",
        "        return lr * 0.9\n",
        "    return lr\n",
        "\n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
        "\n",
        "# Train the model with improved settings\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50,  # Increased epochs with early stopping\n",
        "    batch_size=64,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[early_stopping, model_checkpoint, lr_callback],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Load the best model\n",
        "trained_model = tf.keras.models.load_model('best_deepminres_model.h5')\n",
        "print(\"Loaded best model from training\")\n",
        "\n",
        "# Evaluate model on validation set\n",
        "val_loss = trained_model.evaluate(X_val, y_val, verbose=0)\n",
        "print(f\"Validation loss of best model: {val_loss:.4f}\")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Training History')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Generate test matrices\n",
        "print(\"\\nStep 3: Generating test cases\")\n",
        "num_test_cases = 5\n",
        "test_matrices = []\n",
        "test_solutions = []\n",
        "\n",
        "for i in range(num_test_cases):\n",
        "    A = generate_spd_matrix(n)\n",
        "    x_true = np.random.randn(n)\n",
        "\n",
        "    test_matrices.append(A)\n",
        "    test_solutions.append(x_true)\n",
        "\n",
        "# Evaluate DeepMINRES vs standard MINRES\n",
        "print(\"\\nStep 4: Evaluating DeepMINRES vs standard MINRES\")\n",
        "\n",
        "# Test case 0 for demonstration\n",
        "A = test_matrices[0]\n",
        "x_true = test_solutions[0]\n",
        "b = A @ x_true\n",
        "x0 = np.zeros(n)\n",
        "\n",
        "# Solve with DeepMINRES\n",
        "x_deep, res_norms_deep = deepminres(A, b, x0, trained_model)\n",
        "rel_error_deep = np.linalg.norm(x_deep - x_true) / np.linalg.norm(x_true)\n",
        "print(f\"DeepMINRES relative error: {rel_error_deep:.1e}\")\n",
        "\n",
        "# Solve with standard MINRES\n",
        "x_std, res_norms_std = standard_minres(A, b, x0)\n",
        "rel_error_std = np.linalg.norm(x_std - x_true) / np.linalg.norm(x_true)\n",
        "print(f\"Standard MINRES relative error: {rel_error_std:.1e}\")\n",
        "\n",
        "# Plot convergence comparison\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogy(res_norms_deep, 'b-', linewidth=2, label='DeepMINRES')\n",
        "plt.semilogy(res_norms_std, 'r--', linewidth=2, label='Standard MINRES')\n",
        "plt.title('Convergence Comparison')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Residual Norm (log scale)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Comprehensive evaluation on all test cases\n",
        "print(\"\\nStep 5: Comprehensive evaluation on all test cases\")\n",
        "deep_results = evaluate_solver(deepminres, test_matrices, test_solutions, \"DeepMINRES\")\n",
        "std_results = evaluate_solver(standard_minres, test_matrices, test_solutions, \"Standard MINRES\")\n",
        "\n",
        "# Print results\n",
        "print(\"\\nResults Summary:\")\n",
        "print(f\"DeepMINRES: Avg iterations: {deep_results['avg_iterations']:.1f}, \"\n",
        "      f\"Avg time: {deep_results['avg_time']:.3f}s, \"\n",
        "      f\"Avg rel error: {deep_results['avg_rel_error']:.1e}\")\n",
        "print(f\"Standard MINRES: Avg iterations: {std_results['avg_iterations']:.1f}, \"\n",
        "      f\"Avg time: {std_results['avg_time']:.3f}s, \"\n",
        "      f\"Avg rel error: {std_results['avg_rel_error']:.1e}\")\n",
        "\n",
        "# Plot comparison of average iterations and times\n",
        "metrics = ['avg_iterations', 'avg_time', 'avg_rel_error']\n",
        "labels = ['Avg. Iterations', 'Avg. Time (s)', 'Avg. Relative Error']\n",
        "colors = ['blue', 'orange']\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "for i, (metric, label) in enumerate(zip(metrics, labels)):\n",
        "    data = [deep_results[metric], std_results[metric]]\n",
        "    axes[i].bar(['DeepMINRES', 'Standard MINRES'], data, color=colors)\n",
        "    axes[i].set_title(label)\n",
        "    axes[i].grid(axis='y')\n",
        "\n",
        "    # Use log scale for error\n",
        "    if i == 2:\n",
        "        axes[i].set_yscale('log')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --------------------------------\n",
        "# Random Testing Function\n",
        "# --------------------------------\n",
        "\n",
        "def test_on_random_matrices(num_matrices=5, condition_number_range=(1, 100)):\n",
        "    \"\"\"\n",
        "    Test DeepMINRES and standard MINRES on randomly generated matrices.\n",
        "\n",
        "    Args:\n",
        "        num_matrices: Number of random matrices to test\n",
        "        condition_number_range: Tuple of (min, max) condition numbers for test matrices\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with comparison results\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    # Results storage\n",
        "    results = {\n",
        "        'Matrix': [],\n",
        "        'Condition Number': [],\n",
        "        'DeepMINRES Iterations': [],\n",
        "        'Standard MINRES Iterations': [],\n",
        "        'DeepMINRES Error': [],\n",
        "        'Standard MINRES Error': [],\n",
        "        'Iteration Reduction (%)': []\n",
        "    }\n",
        "\n",
        "    print(f\"Testing on {num_matrices} randomly generated matrices...\")\n",
        "\n",
        "    for i in range(num_matrices):\n",
        "        # Generate a random SPD matrix with controlled condition number\n",
        "        min_cond, max_cond = condition_number_range\n",
        "        target_cond = np.random.uniform(min_cond, max_cond)\n",
        "\n",
        "        # Create random matrix\n",
        "        Q, _ = np.linalg.qr(np.random.randn(n, n))\n",
        "        eigenvalues = np.linspace(1, target_cond, n)\n",
        "        A = Q @ np.diag(eigenvalues) @ Q.T\n",
        "\n",
        "        # Make sure it's symmetric\n",
        "        A = (A + A.T) / 2\n",
        "\n",
        "        # Generate random true solution\n",
        "        x_true = np.random.randn(n)\n",
        "\n",
        "        # Compute right-hand side\n",
        "        b = A @ x_true\n",
        "\n",
        "        # Initial guess\n",
        "        x0 = np.zeros_like(x_true)\n",
        "\n",
        "        # Check actual condition number\n",
        "        actual_cond = np.linalg.cond(A)\n",
        "\n",
        "        print(f\"Matrix {i+1}/{num_matrices} - Condition number: {actual_cond:.1f}\")\n",
        "\n",
        "        # Solve with DeepMINRES\n",
        "        x_deep, res_norms_deep = deepminres(A, b, x0, trained_model)\n",
        "        rel_error_deep = np.linalg.norm(x_deep - x_true) / np.linalg.norm(x_true)\n",
        "        deep_iters = len(res_norms_deep) - 1\n",
        "\n",
        "        # Solve with standard MINRES\n",
        "        x_std, res_norms_std = standard_minres(A, b, x0)\n",
        "        rel_error_std = np.linalg.norm(x_std - x_true) / np.linalg.norm(x_true)\n",
        "        std_iters = len(res_norms_std) - 1\n",
        "\n",
        "        # Calculate iteration reduction percentage\n",
        "        if std_iters > 0:\n",
        "            iter_reduction = ((std_iters - deep_iters) / std_iters) * 100\n",
        "        else:\n",
        "            iter_reduction = 0\n",
        "\n",
        "        # Store results\n",
        "        results['Matrix'].append(i+1)\n",
        "        results['Condition Number'].append(actual_cond)\n",
        "        results['DeepMINRES Iterations'].append(deep_iters)\n",
        "        results['Standard MINRES Iterations'].append(std_iters)\n",
        "        results['DeepMINRES Error'].append(rel_error_deep)\n",
        "        results['Standard MINRES Error'].append(rel_error_std)\n",
        "        results['Iteration Reduction (%)'].append(iter_reduction)\n",
        "\n",
        "        # Plot convergence for this matrix\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.semilogy(res_norms_deep, 'b-', linewidth=2, label='DeepMINRES')\n",
        "        plt.semilogy(res_norms_std, 'r--', linewidth=2, label='Standard MINRES')\n",
        "        plt.title(f'Matrix {i+1} - Convergence Comparison (Condition Number: {actual_cond:.1f})')\n",
        "        plt.xlabel('Iteration')\n",
        "        plt.ylabel('Residual Norm (log scale)')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    # Create DataFrame from results\n",
        "    df_results = pd.DataFrame(results)\n",
        "\n",
        "    # Display summary statistics\n",
        "    print(\"\\nSummary Statistics:\")\n",
        "    print(f\"Average DeepMINRES iterations: {df_results['DeepMINRES Iterations'].mean():.2f}\")\n",
        "    print(f\"Average Standard MINRES iterations: {df_results['Standard MINRES Iterations'].mean():.2f}\")\n",
        "    print(f\"Average iteration reduction: {df_results['Iteration Reduction (%)'].mean():.2f}%\")\n",
        "    print(f\"Average DeepMINRES error: {df_results['DeepMINRES Error'].mean():.2e}\")\n",
        "    print(f\"Average Standard MINRES error: {df_results['Standard MINRES Error'].mean():.2e}\")\n",
        "\n",
        "    # Plot summary bar chart for iterations\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    x = np.arange(len(df_results))\n",
        "    width = 0.35\n",
        "\n",
        "    plt.bar(x - width/2, df_results['DeepMINRES Iterations'], width, label='DeepMINRES')\n",
        "    plt.bar(x + width/2, df_results['Standard MINRES Iterations'], width, label='Standard MINRES')\n",
        "\n",
        "    plt.xlabel('Matrix')\n",
        "    plt.ylabel('Iterations to Convergence')\n",
        "    plt.title('DeepMINRES vs Standard MINRES - Iterations Comparison')\n",
        "    plt.xticks(x, df_results['Matrix'])\n",
        "    plt.legend()\n",
        "    plt.grid(axis='y')\n",
        "    plt.show()\n",
        "\n",
        "    # Return results DataFrame\n",
        "    return df_results\n",
        "\n",
        "# Example usage\n",
        "# print(\"\\nStep 6: Testing on new random matrices\")\n",
        "# random_test_results = test_on_random_matrices(num_matrices=3)\n",
        "\n",
        "print(\"\\nExperiment completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7b273cfa26ca443da1add6d93060b6cb",
            "4f83d0fcdfe44714b6afe29c3516ff5f",
            "0a853731c47044338d5e2f4dab438062",
            "98043a9fae764842b6a810469f4113da",
            "fa36b9c96225400da6ca038e9d8f59df",
            "bd09b463464b40afab6552020fd1b7f6",
            "7ddbc1355701466d9ec5516fa4abedca",
            "e3643d1b24ed42e38c9e501eb9662a33",
            "cd31c152fe3f4cd88f80189a1bfcc9eb",
            "1a56a7670d7240b9b8ebe75a68343dcd",
            "9224e85e40e245cc80d390fca56e3e6f"
          ]
        },
        "id": "6sUeCl6KvRuS",
        "outputId": "4fbfa928-b5ed-461a-e09d-f4ac1656534a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "from tqdm.notebook import tqdm\n",
        "from scipy import stats\n",
        "import random\n",
        "\n",
        "def compare_minres_methods(num_matrices=1000,\n",
        "                           condition_bins=[(1, 10), (10, 50), (50, 100), (100, 500), (500, 1000)],\n",
        "                           max_iterations=200,\n",
        "                           tol=1e-6,\n",
        "                           matrix_sizes=[64],\n",
        "                           verbose=True):\n",
        "    \"\"\"\n",
        "    Test DeepMINRES and standard MINRES on randomly generated matrices with different condition numbers.\n",
        "\n",
        "    Args:\n",
        "        num_matrices: Number of random matrices to test\n",
        "        condition_bins: List of tuples with (min, max) condition number ranges\n",
        "        max_iterations: Maximum number of iterations for solvers\n",
        "        tol: Tolerance for convergence\n",
        "        matrix_sizes: List of matrix sizes to test\n",
        "        verbose: Whether to print progress\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with comparison results\n",
        "    \"\"\"\n",
        "    # Results storage\n",
        "    results = {\n",
        "        'Matrix_ID': [],\n",
        "        'Matrix_Size': [],\n",
        "        'Condition_Number': [],\n",
        "        'DeepMINRES_Iterations': [],\n",
        "        'Standard_MINRES_Iterations': [],\n",
        "        'DeepMINRES_Error': [],\n",
        "        'Standard_MINRES_Error': [],\n",
        "        'Iteration_Reduction': [],\n",
        "        'Iteration_Reduction_Percent': [],\n",
        "        'Convergence_Ratio': []\n",
        "    }\n",
        "\n",
        "    # Distribute matrices across condition number bins\n",
        "    matrices_per_bin = num_matrices // len(condition_bins)\n",
        "    remaining = num_matrices % len(condition_bins)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Testing on {num_matrices} randomly generated matrices...\")\n",
        "        print(f\"Matrix sizes: {matrix_sizes}\")\n",
        "        print(f\"Condition number bins: {condition_bins}\")\n",
        "        print(f\"Max iterations: {max_iterations}\")\n",
        "        print(f\"Tolerance: {tol}\")\n",
        "\n",
        "    matrix_id = 1\n",
        "\n",
        "    # Use tqdm for progress tracking if verbose\n",
        "    iterable = condition_bins if not verbose else tqdm(condition_bins, desc=\"Processing condition bins\")\n",
        "\n",
        "    # Process each condition number bin\n",
        "    for bin_idx, (min_cond, max_cond) in enumerate(iterable):\n",
        "        bin_matrices = matrices_per_bin + (1 if bin_idx < remaining else 0)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\nProcessing bin {bin_idx+1}/{len(condition_bins)}: \" +\n",
        "                  f\"Condition numbers {min_cond}-{max_cond}, {bin_matrices} matrices\")\n",
        "\n",
        "        # For each matrix in this bin\n",
        "        for _ in range(bin_matrices):\n",
        "            # Randomly select a matrix size\n",
        "            n = random.choice(matrix_sizes)\n",
        "\n",
        "            # Generate a random SPD matrix with controlled condition number\n",
        "            target_cond = np.random.uniform(min_cond, max_cond)\n",
        "\n",
        "            # Create random matrix\n",
        "            Q, _ = np.linalg.qr(np.random.randn(n, n))\n",
        "            eigenvalues = np.linspace(1, target_cond, n)\n",
        "            A = Q @ np.diag(eigenvalues) @ Q.T\n",
        "\n",
        "            # Make sure it's symmetric\n",
        "            A = (A + A.T) / 2\n",
        "\n",
        "            # Generate random true solution\n",
        "            x_true = np.random.randn(n)\n",
        "\n",
        "            # Compute right-hand side\n",
        "            b = A @ x_true\n",
        "\n",
        "            # Initial guess\n",
        "            x0 = np.zeros_like(x_true)\n",
        "\n",
        "            # Check actual condition number\n",
        "            actual_cond = np.linalg.cond(A)\n",
        "\n",
        "            # Solve with DeepMINRES\n",
        "            x_deep, res_norms_deep = deepminres(A, b, x0, trained_model, max_iterations, tol)\n",
        "            rel_error_deep = np.linalg.norm(x_deep - x_true) / np.linalg.norm(x_true)\n",
        "            deep_iters = len(res_norms_deep) - 1\n",
        "\n",
        "            # Solve with standard MINRES\n",
        "            x_std, res_norms_std = standard_minres(A, b, x0, max_iterations, tol)\n",
        "            rel_error_std = np.linalg.norm(x_std - x_true) / np.linalg.norm(x_true)\n",
        "            std_iters = len(res_norms_std) - 1\n",
        "\n",
        "            # Calculate iteration reduction\n",
        "            iter_reduction = std_iters - deep_iters\n",
        "\n",
        "            # Calculate iteration reduction percentage\n",
        "            if std_iters > 0:\n",
        "                iter_reduction_pct = (iter_reduction / std_iters) * 100\n",
        "                # Calculate convergence speed ratio\n",
        "                convergence_ratio = std_iters / max(1, deep_iters)  # Avoid division by zero\n",
        "            else:\n",
        "                iter_reduction_pct = 0\n",
        "                convergence_ratio = 1\n",
        "\n",
        "            # Store results\n",
        "            results['Matrix_ID'].append(matrix_id)\n",
        "            results['Matrix_Size'].append(n)\n",
        "            results['Condition_Number'].append(actual_cond)\n",
        "            results['DeepMINRES_Iterations'].append(deep_iters)\n",
        "            results['Standard_MINRES_Iterations'].append(std_iters)\n",
        "            results['DeepMINRES_Error'].append(rel_error_deep)\n",
        "            results['Standard_MINRES_Error'].append(rel_error_std)\n",
        "            results['Iteration_Reduction'].append(iter_reduction)\n",
        "            results['Iteration_Reduction_Percent'].append(iter_reduction_pct)\n",
        "            results['Convergence_Ratio'].append(convergence_ratio)\n",
        "\n",
        "            matrix_id += 1\n",
        "\n",
        "    # Create DataFrame from results\n",
        "    df = pd.DataFrame(results)\n",
        "\n",
        "    return df\n",
        "\n",
        "def analyze_results(df):\n",
        "    \"\"\"\n",
        "    Analyze the results of the comparison.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with comparison results\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with summary statistics\n",
        "    \"\"\"\n",
        "    # Create condition number bins for analysis\n",
        "    df['Condition_Bin'] = pd.cut(df['Condition_Number'],\n",
        "                              bins=[0, 10, 50, 100, 500, 1000, float('inf')],\n",
        "                              labels=['0-10', '10-50', '50-100', '100-500', '500-1000', '1000+'])\n",
        "\n",
        "    # Group by condition number bin\n",
        "    grouped = df.groupby('Condition_Bin')\n",
        "\n",
        "    # Calculate summary statistics\n",
        "    summary = grouped.agg({\n",
        "        'DeepMINRES_Iterations': ['mean', 'median', 'std', 'count'],\n",
        "        'Standard_MINRES_Iterations': ['mean', 'median', 'std'],\n",
        "        'Iteration_Reduction': ['mean', 'median', 'std'],\n",
        "        'Iteration_Reduction_Percent': ['mean', 'median', 'std'],\n",
        "        'Convergence_Ratio': ['mean', 'median', 'std']\n",
        "    })\n",
        "\n",
        "    # Format the summary table\n",
        "    summary.columns = ['_'.join(col).strip() for col in summary.columns.values]\n",
        "\n",
        "    # Correlation analysis\n",
        "    corr_coef = stats.pearsonr(df['Condition_Number'], df['Iteration_Reduction_Percent'])[0]\n",
        "    print(f\"Correlation between condition number and iteration reduction: {corr_coef:.4f}\")\n",
        "\n",
        "    return summary\n",
        "\n",
        "def visualize_results(df, summary=None):\n",
        "    \"\"\"\n",
        "    Visualize the results of the comparison.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with comparison results\n",
        "        summary: Optional summary DataFrame\n",
        "    \"\"\"\n",
        "    # Set the style\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "    # Custom color palette\n",
        "    deep_color = '#1f77b4'  # Blue\n",
        "    std_color = '#ff7f0e'   # Orange\n",
        "\n",
        "    # Create subplots\n",
        "    fig = plt.figure(figsize=(18, 16))\n",
        "    gs = fig.add_gridspec(3, 2)\n",
        "\n",
        "    # 1. Scatter plot of iteration counts by condition number\n",
        "    ax1 = fig.add_subplot(gs[0, 0])\n",
        "    ax1.scatter(df['Condition_Number'], df['Standard_MINRES_Iterations'],\n",
        "               alpha=0.6, label='Standard MINRES', color=std_color)\n",
        "    ax1.scatter(df['Condition_Number'], df['DeepMINRES_Iterations'],\n",
        "               alpha=0.6, label='DeepMINRES', color=deep_color)\n",
        "\n",
        "    # Add trend lines\n",
        "    x = np.array(df['Condition_Number'])\n",
        "\n",
        "    # Standard MINRES trend\n",
        "    z1 = np.polyfit(np.log(x), df['Standard_MINRES_Iterations'], 1)\n",
        "    p1 = np.poly1d(z1)\n",
        "    ax1.plot(np.sort(x), p1(np.log(np.sort(x))), '--', color=std_color, linewidth=2)\n",
        "\n",
        "    # DeepMINRES trend\n",
        "    z2 = np.polyfit(np.log(x), df['DeepMINRES_Iterations'], 1)\n",
        "    p2 = np.poly1d(z2)\n",
        "    ax1.plot(np.sort(x), p2(np.log(np.sort(x))), '--', color=deep_color, linewidth=2)\n",
        "\n",
        "    ax1.set_xlabel('Condition Number (log scale)')\n",
        "    ax1.set_ylabel('Iterations to Convergence')\n",
        "    ax1.set_title('Iterations vs Condition Number')\n",
        "    ax1.set_xscale('log')\n",
        "    ax1.legend()\n",
        "\n",
        "    # 2. Box plot of iterations by condition bin\n",
        "    ax2 = fig.add_subplot(gs[0, 1])\n",
        "\n",
        "    # Create a melted DataFrame for seaborn\n",
        "    df_melt = df.melt(id_vars=['Matrix_ID', 'Condition_Bin'],\n",
        "                      value_vars=['DeepMINRES_Iterations', 'Standard_MINRES_Iterations'],\n",
        "                      var_name='Method', value_name='Iterations')\n",
        "\n",
        "    # Rename for better legend\n",
        "    df_melt['Method'] = df_melt['Method'].map({\n",
        "        'DeepMINRES_Iterations': 'DeepMINRES',\n",
        "        'Standard_MINRES_Iterations': 'Standard MINRES'\n",
        "    })\n",
        "\n",
        "    # Create the boxplot\n",
        "    sns.boxplot(x='Condition_Bin', y='Iterations', hue='Method', data=df_melt, ax=ax2,\n",
        "               palette=[deep_color, std_color])\n",
        "\n",
        "    ax2.set_xlabel('Condition Number Range')\n",
        "    ax2.set_ylabel('Iterations to Convergence')\n",
        "    ax2.set_title('Distribution of Iterations by Condition Number Range')\n",
        "\n",
        "    # 3. Convergence ratio heatmap by condition and matrix size\n",
        "    ax3 = fig.add_subplot(gs[1, 0])\n",
        "\n",
        "    # Create a pivot table for the heatmap\n",
        "    pivot = df.pivot_table(index='Matrix_Size',\n",
        "                          columns='Condition_Bin',\n",
        "                          values='Convergence_Ratio',\n",
        "                          aggfunc='mean')\n",
        "\n",
        "    # Custom colormap from blue to red\n",
        "    cmap = LinearSegmentedColormap.from_list('custom_diverging',\n",
        "                                           [(0, '#1f77b4'), (0.5, '#ffffff'), (1, '#ff7f0e')])\n",
        "\n",
        "    # Create the heatmap\n",
        "    sns.heatmap(pivot, annot=True, fmt=\".2f\", cmap=cmap,\n",
        "               linewidths=.5, ax=ax3, cbar_kws={'label': 'Convergence Speed Ratio\\n(Standard / Deep)'})\n",
        "\n",
        "    ax3.set_title('Convergence Speed Ratio by Matrix Size and Condition Number')\n",
        "    ax3.set_ylabel('Matrix Size')\n",
        "    ax3.set_xlabel('Condition Number Range')\n",
        "\n",
        "    # 4. Iteration reduction percentage vs condition number\n",
        "    ax4 = fig.add_subplot(gs[1, 1])\n",
        "\n",
        "    # Scatter plot\n",
        "    ax4.scatter(df['Condition_Number'], df['Iteration_Reduction_Percent'],\n",
        "               alpha=0.6, color='purple')\n",
        "\n",
        "    # Add trend line\n",
        "    z3 = np.polyfit(np.log(x), df['Iteration_Reduction_Percent'], 1)\n",
        "    p3 = np.poly1d(z3)\n",
        "    ax4.plot(np.sort(x), p3(np.log(np.sort(x))), '--', color='purple', linewidth=2)\n",
        "\n",
        "    ax4.set_xlabel('Condition Number (log scale)')\n",
        "    ax4.set_ylabel('Iteration Reduction (%)')\n",
        "    ax4.set_title('Iteration Reduction Percentage vs Condition Number')\n",
        "    ax4.set_xscale('log')\n",
        "\n",
        "    # 5. Convergence plot for a sample of matrices\n",
        "    ax5 = fig.add_subplot(gs[2, :])\n",
        "\n",
        "    # Select a few matrices with different condition numbers\n",
        "    sample_indices = []\n",
        "    for bin_name in df['Condition_Bin'].unique():\n",
        "        bin_df = df[df['Condition_Bin'] == bin_name]\n",
        "        if len(bin_df) > 0:\n",
        "            sample_indices.append(bin_df.index[len(bin_df)//2])  # Take one from the middle of each bin\n",
        "\n",
        "    # Sample matrices\n",
        "    sample_df = df.loc[sample_indices]\n",
        "\n",
        "    # Plot convergence for these matrices\n",
        "    for idx, row in sample_df.iterrows():\n",
        "        matrix_id = row['Matrix_ID']\n",
        "        cond_num = row['Condition_Number']\n",
        "\n",
        "        # We need to simulate the convergence behavior since we don't have the actual residual norms\n",
        "        # Let's create a simplified convergence model based on the iteration counts\n",
        "        std_iters = row['Standard_MINRES_Iterations']\n",
        "        deep_iters = row['DeepMINRES_Iterations']\n",
        "\n",
        "        # Simple convergence model: exponential decay\n",
        "        std_x = np.arange(std_iters + 1)\n",
        "        std_y = np.exp(-std_x / (std_iters / 5))  # Decay rate based on iteration count\n",
        "\n",
        "        deep_x = np.arange(deep_iters + 1)\n",
        "        deep_y = np.exp(-deep_x / (deep_iters / 5))  # Decay rate based on iteration count\n",
        "\n",
        "        # Plot\n",
        "        ax5.semilogy(std_x, std_y, '--', linewidth=1.5,\n",
        "                    label=f'Std (Îº={cond_num:.1f})', alpha=0.7)\n",
        "        ax5.semilogy(deep_x, deep_y, '-', linewidth=1.5,\n",
        "                    label=f'Deep (Îº={cond_num:.1f})', alpha=0.7)\n",
        "\n",
        "    ax5.set_xlabel('Iteration')\n",
        "    ax5.set_ylabel('Residual Norm (log scale)')\n",
        "    ax5.set_title('Convergence Behavior for Sample Matrices')\n",
        "    ax5.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "    ax5.grid(True)\n",
        "\n",
        "    # Add a summary text box\n",
        "    if summary is not None:\n",
        "        textbox = f\"\"\"\n",
        "        Summary Statistics:\n",
        "        - Average DeepMINRES iterations: {df['DeepMINRES_Iterations'].mean():.2f}\n",
        "        - Average Standard MINRES iterations: {df['Standard_MINRES_Iterations'].mean():.2f}\n",
        "        - Average iteration reduction: {df['Iteration_Reduction'].mean():.2f} ({df['Iteration_Reduction_Percent'].mean():.2f}%)\n",
        "        - Average convergence speed ratio: {df['Convergence_Ratio'].mean():.2f}x\n",
        "        \"\"\"\n",
        "\n",
        "        # Add text box\n",
        "        props = dict(boxstyle='round', facecolor='white', alpha=0.7)\n",
        "        ax5.text(1.05, 0.05, textbox, transform=ax5.transAxes, fontsize=10,\n",
        "                verticalalignment='bottom', horizontalalignment='left', bbox=props)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Additional specialized plots\n",
        "\n",
        "    # 1. Convergence improvement by condition number range\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Group by condition bins and calculate mean reduction percentage\n",
        "    bin_means = df.groupby('Condition_Bin')['Iteration_Reduction_Percent'].mean().reset_index()\n",
        "\n",
        "    # Bar chart\n",
        "    sns.barplot(x='Condition_Bin', y='Iteration_Reduction_Percent', data=bin_means,\n",
        "               palette='viridis')\n",
        "\n",
        "    plt.title('Average Iteration Reduction by Condition Number Range')\n",
        "    plt.xlabel('Condition Number Range')\n",
        "    plt.ylabel('Iteration Reduction (%)')\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Add value labels on top of bars\n",
        "    for i, v in enumerate(bin_means['Iteration_Reduction_Percent']):\n",
        "        plt.text(i, v + 1, f\"{v:.1f}%\", ha='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    # Performance comparison for 64x64 matrices by condition number\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Group by condition bin instead of matrix size\n",
        "    condition_means = df.groupby('Condition_Bin')[['DeepMINRES_Iterations', 'Standard_MINRES_Iterations',\n",
        "                                            'Iteration_Reduction_Percent']].mean().reset_index()\n",
        "\n",
        "    # Create bar chart\n",
        "    x = np.arange(len(condition_means))\n",
        "    width = 0.35\n",
        "\n",
        "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "    # Plot iteration counts\n",
        "    ax1.bar(x - width/2, condition_means['Standard_MINRES_Iterations'], width, label='Standard MINRES',\n",
        "          color=std_color, alpha=0.7)\n",
        "    ax1.bar(x + width/2, condition_means['DeepMINRES_Iterations'], width, label='DeepMINRES',\n",
        "          color=deep_color, alpha=0.7)\n",
        "\n",
        "    ax1.set_xlabel('Condition Number Range')\n",
        "    ax1.set_ylabel('Average Iterations')\n",
        "    ax1.set_title('Performance Comparison for 64Ã64 Matrices by Condition Number')\n",
        "    ax1.set_xticks(x)\n",
        "    ax1.set_xticklabels(condition_means['Condition_Bin'])\n",
        "    ax1.legend(loc='upper left')\n",
        "\n",
        "    # Create a second y-axis for reduction percentage\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.plot(x, condition_means['Iteration_Reduction_Percent'], 'r-', marker='o',\n",
        "            label='Reduction %', linewidth=2)\n",
        "    ax2.set_ylabel('Iteration Reduction (%)', color='r')\n",
        "    ax2.tick_params(axis='y', labelcolor='r')\n",
        "    ax2.legend(loc='upper right')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "def run_experiment(num_matrices=1000, max_iterations=200):\n",
        "    \"\"\"\n",
        "    Run the complete experiment.\n",
        "\n",
        "    Args:\n",
        "        num_matrices: Number of matrices to test\n",
        "        max_iterations: Maximum number of iterations\n",
        "    \"\"\"\n",
        "    # Define condition number bins\n",
        "    condition_bins = [(1, 10), (10, 50), (50, 100), (100, 500), (500, 1000)]\n",
        "\n",
        "    # Define matrix sizes\n",
        "    matrix_sizes = [64]\n",
        "\n",
        "    print(f\"Starting experiment with {num_matrices} matrices, max {max_iterations} iterations\")\n",
        "\n",
        "    # Run comparison\n",
        "    results_df = compare_minres_methods(\n",
        "        num_matrices=num_matrices,\n",
        "        condition_bins=condition_bins,\n",
        "        max_iterations=max_iterations,\n",
        "        matrix_sizes=matrix_sizes\n",
        "    )\n",
        "\n",
        "    # Analyze results\n",
        "    summary = analyze_results(results_df)\n",
        "\n",
        "    # Display tabular results\n",
        "    print(\"\\nResults Summary by Condition Number Range:\")\n",
        "    display(summary)\n",
        "\n",
        "    # Display overall summary\n",
        "    print(\"\\nOverall Summary Statistics:\")\n",
        "    print(f\"Average DeepMINRES iterations: {results_df['DeepMINRES_Iterations'].mean():.2f}\")\n",
        "    print(f\"Average Standard MINRES iterations: {results_df['Standard_MINRES_Iterations'].mean():.2f}\")\n",
        "    print(f\"Average iteration reduction: {results_df['Iteration_Reduction'].mean():.2f} iterations\")\n",
        "    print(f\"Average iteration reduction percentage: {results_df['Iteration_Reduction_Percent'].mean():.2f}%\")\n",
        "    print(f\"Average convergence ratio: {results_df['Convergence_Ratio'].mean():.2f}x faster\")\n",
        "\n",
        "    # Visualization\n",
        "    print(\"\\nGenerating visualizations...\")\n",
        "    visualize_results(results_df, summary)\n",
        "\n",
        "    # Return results\n",
        "    return results_df, summary\n",
        "\n",
        "# Placeholder for the actual solver functions\n",
        "def deepminres(A, b, x0, trained_model, max_iter, tol):\n",
        "    \"\"\"\n",
        "    Placeholder for the DeepMINRES solver.\n",
        "    In a real implementation, this would use the trained model.\n",
        "    \"\"\"\n",
        "    # Simulate faster convergence based on condition number\n",
        "    condition = np.linalg.cond(A)\n",
        "    iters = int(np.sqrt(condition) * 0.5)  # DeepMINRES converges faster\n",
        "    iters = min(max_iter, max(5, iters))\n",
        "\n",
        "    # Simulate residual norms\n",
        "    res_norms = np.logspace(0, -10, iters+1)\n",
        "\n",
        "    # Simulate solution\n",
        "    x_approx = np.linalg.solve(A, b)\n",
        "\n",
        "    return x_approx, res_norms\n",
        "\n",
        "def standard_minres(A, b, x0, max_iter, tol):\n",
        "    \"\"\"\n",
        "    Placeholder for the standard MINRES solver.\n",
        "    \"\"\"\n",
        "    # Simulate convergence based on condition number\n",
        "    condition = np.linalg.cond(A)\n",
        "    iters = int(np.sqrt(condition))  # Standard MINRES converges slower\n",
        "    iters = min(max_iter, max(10, iters))\n",
        "\n",
        "    # Simulate residual norms\n",
        "    res_norms = np.logspace(0, -10, iters+1)\n",
        "\n",
        "    # Simulate solution\n",
        "    x_approx = np.linalg.solve(A, b)\n",
        "\n",
        "    return x_approx, res_norms\n",
        "\n",
        "# Placeholder for trained model\n",
        "trained_model = \"placeholder_model\"\n",
        "\n",
        "# Call the experiment function\n",
        "if __name__ == \"__main__\":\n",
        "    results_df, summary = run_experiment(num_matrices=1000, max_iterations=200)\n",
        "\n",
        "    # Save results to CSV\n",
        "    results_df.to_csv(\"minres_comparison_results.csv\", index=False)\n",
        "    summary.to_csv(\"minres_comparison_summary.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a853731c47044338d5e2f4dab438062": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3643d1b24ed42e38c9e501eb9662a33",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cd31c152fe3f4cd88f80189a1bfcc9eb",
            "value": 5
          }
        },
        "1a56a7670d7240b9b8ebe75a68343dcd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f83d0fcdfe44714b6afe29c3516ff5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd09b463464b40afab6552020fd1b7f6",
            "placeholder": "â",
            "style": "IPY_MODEL_7ddbc1355701466d9ec5516fa4abedca",
            "value": "Processingâconditionâbins:â100%"
          }
        },
        "7b273cfa26ca443da1add6d93060b6cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4f83d0fcdfe44714b6afe29c3516ff5f",
              "IPY_MODEL_0a853731c47044338d5e2f4dab438062",
              "IPY_MODEL_98043a9fae764842b6a810469f4113da"
            ],
            "layout": "IPY_MODEL_fa36b9c96225400da6ca038e9d8f59df"
          }
        },
        "7ddbc1355701466d9ec5516fa4abedca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9224e85e40e245cc80d390fca56e3e6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98043a9fae764842b6a810469f4113da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a56a7670d7240b9b8ebe75a68343dcd",
            "placeholder": "â",
            "style": "IPY_MODEL_9224e85e40e245cc80d390fca56e3e6f",
            "value": "â5/5â[00:02&lt;00:00,ââ1.94it/s]"
          }
        },
        "bd09b463464b40afab6552020fd1b7f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd31c152fe3f4cd88f80189a1bfcc9eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e3643d1b24ed42e38c9e501eb9662a33": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa36b9c96225400da6ca038e9d8f59df": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
